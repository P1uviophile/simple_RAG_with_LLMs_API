import time
import os
import gradio as gr
from langchain.document_loaders import DirectoryLoader
from langchain.llms import ChatGLM
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from neo4j import search

# è®¾ç½®åˆå§‹åŒ–æ˜¯å¦ä½¿ç”¨çŸ¥è¯†å›¾è°±æ•°æ®åº“æœç´¢ç›¸å…³çŸ¥è¯†ç‚¹
neo4j = False
if neo4j:
    neo4j_use = "çŸ¥è¯†å›¾è°±å·²å¯åŠ¨"
else:
    neo4j_use = "çŸ¥è¯†å›¾è°±å·²å…³é—­"


def load_documents(directory="documents"):
    loader = DirectoryLoader(directory)
    documents = loader.load()
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)
    return split_docs


def load_embedding_model(local_model_path):
    encode_kwargs = {"normalize_embeddings": False}
    # ä½¿ç”¨cpu
    # model_kwargs = {"device": "cpu"}
    # cuda==11.8 pytorch==2.1.0 å¯ç”¨
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(
        model_name=local_model_path,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db


def add_text(history, text):
    history += [(text, None)]
    print(history)
    return history, gr.update(value="", interactive=False)


def add_file(history, file):
    global qa
    directory = os.path.dirname(file.name)
    documents = load_documents(directory)
    db = store_chroma(documents, embeddings)
    retriever = db.as_retriever()
    qa.retriever = retriever
    history = history + [((file.name,), None)]
    return history


def bot(history):
    global neo4j, neo4j_use
    if not history:
        # history åˆ—è¡¨ä¸ºç©ºçš„å¤„ç†ï¼Œå¯ä»¥è¿”å›ä¸€ä¸ªé»˜è®¤å€¼æˆ–è€…ä¸æ‰§è¡Œä»»ä½•æ“ä½œ
        return history
    message = history[-1][0]
    # print(message)
    search_ans = False
    if neo4j:
        search_ans = search.search_relate(message)
    extra = ""
    if search_ans:
        extra = "\n\n-------------------------------------------\n\nä»¥ä¸‹æ˜¯æ ¹æ®ä½ çš„æé—®æ¨èçš„çŸ¥è¯†ç‚¹:\n" + search_ans
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
    elif (message == "æ‰“å¼€çŸ¥è¯†å›¾è°±") or (message == "å…³é—­çŸ¥è¯†å›¾è°±"):
        response = neo4j_use
    else:
        response = qa({"query": message})['result']
        response += extra
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.01)
        yield history


# å®šä¹‰æŒ‰é’®ç‚¹å‡»äº‹ä»¶å¤„ç†å‡½æ•°
def btn_neo4j_click(history):
    global neo4j_use, neo4j
    if neo4j_use == "çŸ¥è¯†å›¾è°±å·²å…³é—­":
        neo4j = True
        neo4j = search.connect_neo4j(neo4j)
    else:
        neo4j = False
        neo4j = search.connect_neo4j(neo4j)
    if neo4j:
        neo4j_use = "çŸ¥è¯†å›¾è°±å·²å¯åŠ¨"
        neo4j__use = "æ‰“å¼€çŸ¥è¯†å›¾è°±"
    else:
        neo4j_use = "çŸ¥è¯†å›¾è°±å·²å…³é—­"  # åˆ‡æ¢å˜é‡å€¼
        neo4j__use = "å…³é—­çŸ¥è¯†å›¾è°±"
    btn_neo4j.value = neo4j_use
    print("çŸ¥è¯†å›¾è°±çŠ¶æ€:", neo4j_use)
    history += [(neo4j__use, None)]
    # print(history)
    return history


if __name__ == "__main__":
    # åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹
    embeddings = load_embedding_model(local_model_path='text2vec-base-chinese')
    # åŠ è½½å‘é‡æ•°æ®åº“,è‹¥æ²¡æœ‰åˆ™è¯»å–documentsä¸‹æ–‡ä»¶åˆ›å»ºå‘é‡æ•°æ®åº“
    if not os.path.exists('VectorStore'):
        documents = load_documents()
        db = store_chroma(documents, embeddings)
    else:
        db = Chroma(persist_directory='VectorStore', embedding_function=embeddings)
    # ä½¿ç”¨æœ¬åœ°apiæä¾›çš„å¤§æ¨¡å‹æœåŠ¡
    llm = ChatGLM(
        endpoint_url='http://127.0.0.1:8000',
        max_token=80000,
        top_p=0.9
    )
    # åˆ›å»ºqaé—®ç­”é“¾
    QA = PromptTemplate.from_template(
        """æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰å†…å®¹å›ç­”é—®é¢˜ã€‚
    å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
    ç­”æ¡ˆæœ€å¤š400ä¸ªå­—

    {context}

    é—®é¢˜ï¼š{question}

    """)
    retriever = db.as_retriever()
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        verbose=True,
        chain_type_kwargs={"prompt": QA}
    )
    # è®¾ç½®å‰ç«¯äº¤äº’é¡µé¢
    with gr.Blocks() as demo:
        # è®¾ç½®é¡µé¢æ˜¾ç¤º
        chatbot = gr.Chatbot(
            [],
            elem_id="AIåŠ©æ‰‹",
            bubble_full_width=False,
            avatar_images=(None, (os.path.join(os.path.dirname(__file__), "bot.jpg"))),
        )
        # è®¾ç½®è¾“å…¥
        with gr.Row():
            query = gr.Textbox(
                scale=4,
                show_label=False,
                placeholder="è¾“å…¥é—®é¢˜å¹¶æŒ‰ä¸‹å›è½¦é”®æäº¤",
                container=False,
            )
            btn = gr.UploadButton("ğŸ“ä¸Šä¼ å¤–æŒ‚æ•°æ®åº“", file_types=['txt'])
            btn_neo4j = gr.Button(value="å¼€å…³çŸ¥è¯†å›¾è°±")
        # å¤„ç†å¼€å…³çŸ¥è¯†å›¾è°±é€»è¾‘
        neo4j_msg = btn_neo4j.click(btn_neo4j_click, [chatbot], [chatbot], queue=False).then(
            bot, chatbot, chatbot
        )
        # å¤„ç†è¾“å…¥è¾“å‡ºæ•°æ®
        query_msg = query.submit(add_text, [chatbot, query], [chatbot, query], queue=False).then(
            bot, chatbot, chatbot
        )
        query_msg.then(lambda: gr.update(interactive=True), None, [query], queue=False)
        file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
            bot, chatbot, chatbot
        )

    demo.queue()
    demo.launch()
